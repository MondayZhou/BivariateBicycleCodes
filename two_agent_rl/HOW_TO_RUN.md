# How to Run Tests and Evaluations - Step by Step

## üéØ Choose Your Path

### Path A: One-Command Test (Easiest) ‚≠ê
**For:** First-time users, quick verification
**Time:** 5 minutes
**Quality:** Demo results

```bash
cd two_agent_rl
./run_quick_test.sh    # Linux/Mac
# or
python run_experiments.py --mode quick   # Any OS
```

### Path B: Full Experiment (Recommended) üî¨
**For:** Research, publication-quality results
**Time:** 2-4 hours
**Quality:** High-quality results

```bash
cd two_agent_rl
python run_experiments.py --mode full
```

### Path C: Step-by-Step Manual (Most Control) üõ†Ô∏è
**For:** Advanced users, custom configurations
**Time:** Varies
**Quality:** Customizable

See detailed steps below.

---

## üìã Prerequisites

### 1. Check Python Version
```bash
python --version  # Should be 3.8 or higher
```

### 2. Install Dependencies
```bash
cd two_agent_rl
pip install -r requirements.txt
```

Or install manually:
```bash
pip install torch torch-geometric numpy scipy matplotlib ldpc
```

### 3. Verify Installation
```bash
python -c "import torch, torch_geometric, numpy, scipy, matplotlib; print('‚úì All packages installed')"
```

---

## üöÄ Running Experiments

### Method 1: Quick Demo (Recommended for First Try)

**What it does:**
- Trains a small two-agent decoder (5,000 steps)
- Evaluates on 2 error rates (50 trials each)
- Compares with BP-OSD baseline
- Generates plots and reports

**How to run:**
```bash
python run_experiments.py --mode quick
```

**Expected output:**
```
======================================================================
TWO-AGENT RL DECODER - EXPERIMENTAL PIPELINE
======================================================================
Mode: quick
Code: [[144, ?, ?]] (m=6, ell=12)
Device: cpu
Save path: ./results/experiment_20241028_123456
======================================================================

======================================================================
STEP 1: Training Two-Agent RL Decoder
======================================================================
Starting Multi-Agent PPO Training...
Target timesteps: 5000

--- Update 10 ---
Timesteps: 500/5000
Mean reward: 15.23
Success rate: 45.2%
...

======================================================================
STEP 3: Evaluating All Decoders
======================================================================

Testing error rate p = 0.0010
  Trial 50/50: LER = 0.0020, Success = 0.9800

Results for p = 0.0010:
  Logical Error Rate: 0.0020
  Success Rate: 0.9800
  Avg Decode Time: 0.001523s

======================================================================
DECODER COMPARISON SUMMARY
======================================================================

Error Rate p = 0.0010
--------------------------------------------------------------------------------
Decoder              LER          Success      Time (ms)    Syn Sat
--------------------------------------------------------------------------------
bposd                0.0023       0.9977       0.52         0.9977
two_agent            0.0020       0.9980       1.52         0.9980
--------------------------------------------------------------------------------

======================================================================
EXPERIMENT COMPLETE!
======================================================================

Results saved to: ./results/experiment_20241028_123456

Generated files:
  - config.json (experiment configuration)
  - comparison_results.json (numerical results)
  - comparison_report.txt (text summary)
  - error_rate_comparison.png (LER plot)
  - success_rate_comparison.png (success plot)
  - two_agent_final.pt (trained model)
```

**Where to find results:**
```
./results/experiment_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ config.json                    # Settings used
‚îú‚îÄ‚îÄ comparison_results.json        # All numerical data
‚îú‚îÄ‚îÄ comparison_report.txt          # Human-readable summary
‚îú‚îÄ‚îÄ error_rate_comparison.png      # Main plot
‚îú‚îÄ‚îÄ success_rate_comparison.png    # Success plot
‚îî‚îÄ‚îÄ two_agent_final.pt            # Trained model
```

---

### Method 2: Full Experiment (For Publication)

**What it does:**
- Trains for 100,000 steps (1-2 hours)
- Evaluates on 5 error rates (1,000 trials each)
- Includes hybrid decoder
- Comprehensive analysis

**How to run:**
```bash
python run_experiments.py --mode full
```

**Options:**
```bash
# Use GPU (much faster if available)
python run_experiments.py --mode full --device cuda

# More training for better results
python run_experiments.py --mode full --two_agent_timesteps 200000

# Larger network
python run_experiments.py --mode full --hidden_dim 256 --num_gnn_layers 6
```

---

### Method 3: Evaluate Pretrained Model

If you already have a trained model:

```bash
python run_experiments.py \
    --mode evaluate \
    --load_two_agent ./results/experiment_XXX/two_agent_final.pt \
    --eval_trials 1000
```

---

### Method 4: Manual Step-by-Step

#### Step 1: Train Two-Agent Decoder

```bash
python train.py \
    --decoder_type two_agent \
    --total_timesteps 50000 \
    --error_rate 0.001 \
    --save_path ./checkpoints/my_training
```

**Monitor progress:**
Training prints updates every 10 iterations showing:
- Timesteps completed
- Mean reward (should increase)
- Success rate (should increase)
- Logical error rate (should decrease)
- Loss values (should decrease)

**What to look for:**
- ‚úì Success rate increasing (good)
- ‚úì Logical error rate decreasing (good)
- ‚úó Reward not changing (need more training)
- ‚úó Loss exploding (reduce learning rate)

#### Step 2: Evaluate Your Model

Create a Python script `my_evaluation.py`:

```python
from experiments import DecoderBenchmark
from agent_architecture import TwoAgentDecoder
import torch

# Load model
decoder = TwoAgentDecoder(m=6, ell=12)
checkpoint = torch.load('./checkpoints/my_training/final_model.pt')
decoder.agent_left.load_state_dict(checkpoint['agent_left'])
decoder.agent_right.load_state_dict(checkpoint['agent_right'])

# Create benchmark
benchmark = DecoderBenchmark(m=6, ell=12, device='cpu')

# Evaluate
results = benchmark.evaluate_decoder(
    decoder,
    decoder_type='two_agent',
    error_rates=[0.0001, 0.0005, 0.001, 0.002, 0.005],
    num_trials=1000,
    verbose=True
)

# Print results
print("\n=== RESULTS ===")
for i, p in enumerate(results['error_rates']):
    ler = results['logical_error_rates'][i]
    success = results['success_rates'][i]
    print(f"p={p:.4f}: LER={ler:.4f}, Success={success:.4f}")

# Save
import json
with open('my_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

Run it:
```bash
python my_evaluation.py
```

#### Step 3: Compare with Baselines

```python
from experiments import DecoderBenchmark

benchmark = DecoderBenchmark(m=6, ell=12)

# Compare all decoders
comparison = benchmark.compare_decoders(
    two_agent_decoder=decoder,
    error_rates=[0.001, 0.002, 0.005],
    num_trials=500,
    include_bposd=True,
    include_single_agent=False  # Skip if not trained
)

# Print summary table
benchmark.print_summary(comparison)

# Save results
benchmark.save_results(comparison, 'comparison.json')
```

#### Step 4: Generate Plots

```python
from visualize import DecoderComparison, load_results

# Load results
results = load_results('comparison.json')

# Create visualizations
vis = DecoderComparison()

# Plot 1: Logical Error Rate
vis.plot_error_rate_comparison(
    results,
    save_path='ler_comparison.png'
)

# Plot 2: Success Rate
vis.plot_success_rate_comparison(
    results,
    save_path='success_comparison.png'
)

print("Plots saved!")
```

---

## üìä Understanding Results

### Key Metrics Explained

**1. Logical Error Rate (LER)** ‚≠ê Most Important
- **What:** Probability of uncorrectable error
- **Lower is better**
- **Goal:** LER < 0.01 at p=0.001

**2. Success Rate**
- **What:** Percentage of correctly decoded instances
- **Higher is better**
- **Goal:** Success > 95% at target error rate

**3. Decode Time**
- **What:** Average time per decoding attempt
- **Lower is better for real-time applications**
- **Trade-off:** Accuracy vs speed

**4. Syndrome Satisfaction Rate**
- **What:** Percentage where all syndrome checks pass
- **Should be close to 100%**
- **If low:** Decoder isn't working properly

### Interpreting Comparison Tables

```
Error Rate p = 0.0010
--------------------------------------------------------------------------------
Decoder              LER          Success      Time (ms)    Syn Sat
--------------------------------------------------------------------------------
bposd                0.0023       0.9977       0.52         0.9977
two_agent            0.0015       0.9985       1.48         0.9985
hybrid               0.0014       0.9986       0.68         0.9986
```

**What this means:**
- ‚úì Two-agent has **35% lower LER** than BP-OSD (0.0015 vs 0.0023)
- ‚úì Hybrid achieves best LER (0.0014) with reasonable speed (0.68ms)
- ‚úì BP-OSD is fastest but least accurate
- ‚úì All have high syndrome satisfaction (>99.7%)

**Conclusion:** Hybrid decoder is best overall!

### Reading the Plots

**Error Rate Comparison Plot (log-log scale)**
- X-axis: Physical error rate (p)
- Y-axis: Logical error rate (LER)
- **Good:** Two-agent line below BP-OSD line
- **Great:** Large gap at higher error rates

**Success Rate Plot**
- X-axis: Physical error rate (p)
- Y-axis: Success percentage
- **Good:** Two-agent maintains high success at higher p
- **Bad:** Curves drop too early

---

## üîç Troubleshooting

### Problem: "ModuleNotFoundError: No module named 'ldpc'"

**Solution:**
```bash
pip install ldpc
```

If that fails:
```bash
pip install cython
pip install git+https://github.com/quantumgizmos/ldpc.git
```

### Problem: Training is very slow

**Solutions:**
1. Use GPU: `--device cuda`
2. Reduce timesteps: `--two_agent_timesteps 10000`
3. Smaller network: `--hidden_dim 64 --num_gnn_layers 2`
4. Fewer episodes: `--episodes_per_update 5`

### Problem: "CUDA out of memory"

**Solutions:**
1. Use CPU: `--device cpu`
2. Reduce batch size: `--episodes_per_update 5`
3. Smaller network: `--hidden_dim 64`

### Problem: Results look random / No learning

**Causes & Solutions:**
1. **Training too short**
   - Solution: Use `--mode full` instead of `quick`
   - Need: At least 50,000 timesteps

2. **Learning rate too high**
   - Solution: `--lr 1e-4` instead of default 3e-4

3. **Reward function issues**
   - Solution: Try different reward type: `--reward_type dense`

4. **Environment setup issues**
   - Check: Are errors being generated? (syndrome weight > 0)
   - Check: Are corrections being applied?

### Problem: BP-OSD comparison fails

**Solution:**
Make sure `ldpc` is installed:
```bash
pip show ldpc
```

Or skip BP-OSD comparison:
```bash
python run_experiments.py --mode quick --include_bposd False
```

---

## üéØ Common Use Cases

### Use Case 1: Quick Verification
**Goal:** Check if everything works
**Time:** 5 minutes

```bash
python run_experiments.py --mode quick
```

### Use Case 2: Research Experiment
**Goal:** Publication-quality results
**Time:** 2-4 hours

```bash
python run_experiments.py --mode full --device cuda
```

### Use Case 3: Test Different Code Sizes
**Goal:** Understand scaling
**Time:** 30 minutes per size

```bash
# Small code
python run_experiments.py --mode quick --m 3 --ell 6

# Medium code (default)
python run_experiments.py --mode quick --m 6 --ell 12

# Large code
python run_experiments.py --mode quick --m 12 --ell 12
```

### Use Case 4: Hyperparameter Tuning
**Goal:** Find best settings
**Time:** Several hours

```bash
# Try different learning rates
for lr in 1e-4 3e-4 1e-3; do
    python train.py --lr $lr --save_path ./checkpoints/lr_${lr}
done

# Compare results
python compare_checkpoints.py ./checkpoints/lr_*
```

### Use Case 5: Production Deployment
**Goal:** Best speed/accuracy trade-off
**Time:** 4 hours (training) + validation

```bash
# 1. Train high-quality RL model
python train.py --total_timesteps 200000 --device cuda

# 2. Create and test hybrid decoder
python run_experiments.py \
    --mode evaluate \
    --load_two_agent ./checkpoints/final_model.pt \
    --include_hybrid True

# 3. Deploy hybrid decoder (combines BP-OSD speed + RL accuracy)
```

---

## üìà Expected Performance Timeline

| Training Time | Success Rate | LER @ p=0.001 | Notes |
|--------------|--------------|---------------|-------|
| 5 min (1k steps) | ~50% | ~0.010 | Random guessing |
| 15 min (5k steps) | ~70% | ~0.005 | Starting to learn |
| 1 hour (25k steps) | ~85% | ~0.003 | Decent performance |
| 2 hours (50k steps) | ~92% | ~0.002 | Good performance |
| 4 hours (100k steps) | ~95% | ~0.0015 | Near-optimal ‚≠ê |
| 8 hours (200k steps) | ~97% | ~0.0012 | Excellent |

**Recommendation:** Train for at least 50k steps (2 hours) for meaningful results.

---

## üìù Next Steps After Running

### If results look good:
1. ‚úÖ Save your trained models
2. ‚úÖ Document hyperparameters used
3. ‚úÖ Test on different error rates
4. ‚úÖ Try the hybrid decoder
5. ‚úÖ Compare with other codes

### If results look bad:
1. ‚ùì Did you train long enough? (need 50k+ steps)
2. ‚ùì Check training curves - is reward increasing?
3. ‚ùì Try different hyperparameters
4. ‚ùì Verify environment setup
5. ‚ùì See troubleshooting section

---

## üí° Pro Tips

1. **Start small:** Always run `--mode quick` first to verify setup
2. **Monitor training:** Watch the success rate - it should increase
3. **Save checkpoints:** Use `--save_interval 50` to save progress
4. **Use GPU:** 5-10√ó faster training with `--device cuda`
5. **Multiple runs:** Train multiple times and average results
6. **Visualize often:** Check plots to understand behavior
7. **Read the theory:** `THEORETICAL_ANALYSIS.md` explains why it works

---

## üìû Getting Help

1. **Check these files first:**
   - `QUICKSTART.md` - Quick reference
   - `README.md` - Detailed documentation
   - `THEORETICAL_ANALYSIS.md` - Theory
   - `example_usage.py` - Code examples

2. **Common questions answered in:**
   - Troubleshooting section above
   - Comments in code files
   - Docstrings in functions

3. **Still stuck?**
   - Check error messages carefully
   - Try the minimal example in `example_usage.py`
   - Verify all dependencies are installed

---

## ‚úÖ Success Checklist

Before running experiments, verify:
- [ ] Python 3.8+ installed
- [ ] All dependencies installed (`pip install -r requirements.txt`)
- [ ] In the correct directory (`cd two_agent_rl`)
- [ ] Enough disk space (~1GB for results)
- [ ] Enough time (5 min for quick, 2-4 hours for full)

After running, you should have:
- [ ] `comparison_results.json` file with numerical data
- [ ] `error_rate_comparison.png` plot
- [ ] `success_rate_comparison.png` plot
- [ ] `comparison_report.txt` text summary
- [ ] Two-agent LER < BP-OSD LER (if trained enough)

---

**Ready to run?** Start with:
```bash
python run_experiments.py --mode quick
```

Good luck! üöÄ
